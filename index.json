[{"content":" Colophon # Written in Obsidian, built with Hugo, styled with Blowfish, and hosted on GitHub Pages. Want to peek under the hood? Check the repo.\nAcknowledgements # Homepage: Beautiful Matterhorn image courtesy of u/Cursed-Table-3229 – much appreciated! AI disclaimer # Every word on this site comes straight from my brain to my fingertips – no AI ghostwriting here! I do, however, use AI as my digital proofreader to catch typos and smooth out awkward sentences. If something comes back sounding too robotic or just\u0026hellip; not me, I scrap it and start over. And yes, I really do love my em dashes – they\u0026rsquo;re everywhere in my daily writing!\n","date":"21 July 2025","externalUrl":null,"permalink":"/about/","section":"byKaj","summary":"","title":"About","type":"page"},{"content":" Overview # Let\u0026rsquo;s face it: GPUs are too valuable to leave idle. This guide shows you how to squeeze every drop of performance from your Intel iGPU by sharing it across your entire virtualization stack – from Proxmox VE host to Kubernetes pods. Your media servers will thank you, your power bill will love you, and yes, you\u0026rsquo;ll still have a console to admire your work. Just don\u0026rsquo;t expect to train your next AI model with this setup – leave those workloads to the big GPUs.\nWhat we\u0026rsquo;re trying to accomplish: graph LR; A[Proxmox VE Host] --\u003e |Passthrough| B[Kubernetes Node] B --\u003e |Passthrough| C[Pod] C --\u003e |Request| D[Container] I\u0026rsquo;ve tested this guide across Proxmox VE 8.4.1 hosts powered by Intel Core i5-8500T (Coffee Lake) and Intel Core i7-6700K (Skylake) processors. The Kubernetes side covers both K3s nodes running on Ubuntu 24.04 LTS (Noble) cloud images and K8s nodes on Talos 1.10.\nPrerequisites # CPU Generation # First things first – not all Intel CPUs can play this game. GVT-g Split Passthrough is exclusive to Intel Core generations 5 to 10. Supported CPU families:\nBroadwell (5th gen) Skylake (6th gen) Kaby Lake (7th gen) Coffee Lake (8th gen) Comet Lake (10th gen) 12th gen and beyond? You\u0026rsquo;re in luck! Forget this guide and explore vGPU/SR-IOV – it splits your GPU into 7 devices! 11th gen (Tiger Lake)? The forgotten generation. Neither GVT-g nor SR-IOV will work for you. Sorry.\nSecure Boot # Hardware passthrough and Secure Boot don\u0026rsquo;t play nice together – you\u0026rsquo;ll need to disable Secure Boot in the virtual machine for this to work. I\u0026rsquo;ll show you how in this guide. However, if you\u0026rsquo;re running an OS that requires Secure Boot (like certain Talos images), you\u0026rsquo;re facing a tough choice: reinstall with a non-Secure Boot image and rebuild your cluster from scratch.\nBIOS Virtualization Settings # Before anything else works, you\u0026rsquo;ll need to enable the virtualization trinity in your BIOS:\nVT-d (Intel Virtualization for Directed I/O) IOMMU (Input-Output Memory Management Unit) VT-x (Intel Virtualization Technology) And any other virtualization-related option you can find You can do this in Step 4 where you have to reboot your host anyway.\nDisable VM Autostart (Optional) # To make your life easier, disable the autostart for the Kubernetes node VMs in Proxmox VE so they stay in a stopped state after the host reboot. Re-enable it when you\u0026rsquo;re finished with this guide.\nOk, with that out of the way, let\u0026rsquo;s get started.\n","date":"26 May 2025","externalUrl":null,"permalink":"/guides/igpu-passthrough/overview/","section":"Guides","summary":"","title":"Overview \u0026 Prerequisites","type":"guides"},{"content":"Access your Proxmox host via SSH or the web GUI\u0026rsquo;s Shell option.\nStep 1: Enable IOMMU and Intel GVT-g # Edit the GRUB configuration file /etc/default/grub and update the GRUB_CMDLINE_LINUX_DEFAULT variable to:\nGRUB_CMDLINE_LINUX_DEFAULT=\u0026#34;intel_iommu=on iommu=pt i915.enable_gvt=1 pcie_acs_override=downstream,multifunction\u0026#34; This is why you need this: intel_iommu=on iommu=pt i915.enable_gvt=1 enables the core trio of IOMMU, passthrough, and GVT-g (mediated devices) functionality. The pcie_acs_override=downstream,multifunction parameter does the heavy lifting by forcing hardware into separate IOMMU groups. No more dealing with device clusters – you get individual control over each piece of hardware, plus the sweet benefit of a stable, non-crashing host.\nApply these new boot parameters by running:\nsudo proxmox-boot-tool refresh Step 2: Load Required Kernel Modules # Those parameters won\u0026rsquo;t work their magic without the right kernel modules loaded first. Add these to /etc/modules:\n# Modules required for PCI passthrough vfio vfio_iommu_type1 vfio_pci vfio_virqfd # Modules required for Intel GVT-g Split kvmgt vfio_mdev i915 Step 3: Update Boot Image # Regenerate the boot image to include the new modules:\nsudo update-initramfs -u -k all Step 4: Reboot # WARNING\nMake sure you drain your Kubernetes nodes on the Proxmox host before rebooting! Restart your Proxmox host for all changes to take effect:\nsudo reboot While rebooting, make sure you enabled all virtualization options in your BIOS like described here.\nStep 5: Verify Configuration After Reboot # After reboot, verify the devices are properly separated into different IOMMU groups. Replace \u0026lt;node\u0026gt; with your actual PVE node name:\nsudo pvesh get /nodes/\u0026lt;node\u0026gt;/hardware/pci --pci-class-blacklist \u0026#34;\u0026#34; Your output should look similar to this:\n┌──────────┬────────┬──────────────┬────────────┬────────┬────────────────────────────────────── │ class │ device │ id │ iommugroup │ vendor │ device_name ╞══════════╪════════╪══════════════╪════════════╪════════╪══════════════════════════════════════ ... │ 0x020000 │ 0x154d │ 0000:01:00.0 │ 10 │ 0x8086 │ Ethernet 10G 2P X520 Adapter ├──────────┼────────┼──────────────┼────────────┼────────┼────────────────────────────────────── │ 0x020000 │ 0x154d │ 0000:01:00.1 │ 11 │ 0x8086 │ Ethernet 10G 2P X520 Adapter ├──────────┼────────┼──────────────┼────────────┼────────┼────────────────────────────────────── │ 0x030000 │ 0x3e92 │ 0000:00:02.0 │ 0 │ 0x8086 │ CoffeeLake-S GT2 [UHD Graphics 630] ├──────────┼────────┼──────────────┼────────────┼────────┼────────────────────────────────────── │ 0x040380 │ 0xa348 │ 0000:00:1f.3 │ 9 │ 0x8086 │ Cannon Lake PCH cAVS ... Take note of the GPU ID (which is 0000:00:02.0 in this example), as you\u0026rsquo;ll use it in the next step.\nStep 6: Confirm GVT-g Support # Verify that GVT-g split is working and mediated devices are available. Replace \u0026lt;gpu id\u0026gt; with your GPU\u0026rsquo;s ID from the previous step:\nsudo ls /sys/bus/pci/devices/\u0026lt;gpu id\u0026gt;/mdev_supported_types You should see the following output:\ni915-GVTg_V5_4 i915-GVTg_V5_8 Step 7: Configure Additional Hosts # If you have multiple Proxmox VE hosts with GPUs, repeat these steps on each host.\nTIP\nIf you have a Proxmox VE cluster, you can create a resource mapping (Datacenter → Resource Mappings). This mapping can then be added to your VM hardware instead of a directly connected raw device. Resource Mappings in the Datacenter view ","date":"26 May 2025","externalUrl":null,"permalink":"/guides/igpu-passthrough/configuring-proxmox/","section":"Guides","summary":"","title":"Configuring the Proxmox hosts","type":"guides"},{"content":"Time to add the GPU! For this your Kubernetes node VM needs to be powered off first. Hot-plugging isn\u0026rsquo;t an option here. If you disabled the autostart option, it should still be powered off after the last host reboot.\nStep 1: Configure GPU Passthrough in Proxmox # Add the GPU via a mediated device (MDev Type) to your Kubernetes node VM using either a resource mapping or as a raw device: Adding the device to the VM Overview of the VM configuration Important: Ensure that vIOMMU in the Machine configuration is set to Default (None).\nStep 2: Disable Secure Boot # Two ways to do this.\nMethod 1: EFI Disk Replacement # Simply remove the current EFI disk and add a new one, ensuring Pre-Enroll keys is disabled: EFI disk with the Pre-Enroll keys option disabled Method 2: BIOS Configuration # Alternatively, disable Secure Boot through the VM\u0026rsquo;s BIOS:\nStart the VM and quickly open the console Press Esc repeatedly during boot to enter the BIOS Navigate to Device Manager → Secure Boot and disable it (multiple confirmations required) Save settings and reboot Step 3: Install GPU Drivers # Your approach depends on your OS.\nFor Talos Systens # Add the official i915 system extension to your installation. Extension management is beyond this guide\u0026rsquo;s scope, but here\u0026rsquo;s what you need:\ncustomization: systemExtensions: officialExtensions: - siderolabs/i915 For Minimal or Cloud-Based Installations # SSH into your node and install the GPU drivers, reboot afterwards:\nsudo apt install linux-generic -y sudo reboot For Other Distributions # Most full distributions include these drivers by default. Verify that linux-generic is installed.\nStep 4: Verify GPU Passthrough # Let\u0026rsquo;s confirm your GPU made it through.\nVerification on Talos # talosctl -n \u0026lt;node-name\u0026gt; ls /dev/dri\t# Use your actual node name Expected output:\nNODE NAME \u0026lt;node-name\u0026gt; . \u0026lt;node-name\u0026gt; by-path \u0026lt;node-name\u0026gt; card0\t# Your GPU! \u0026lt;node-name\u0026gt; renderD128\t# The render device Verification on Other Systems # Log into your node and execute:\nls /dev/dri Should show:\nby-path card0 renderD128\t# card0 is your GPU! Step 5: Configure Additional Nodes # Rinse and repeat! Apply these steps to every Kubernetes node that needs GPU support.\n","date":"26 May 2025","externalUrl":null,"permalink":"/guides/igpu-passthrough/adding-the-gpu/","section":"Guides","summary":"","title":"Adding the GPU to the Kubernetes Nodes","type":"guides"},{"content":"You have everything in place to make Kubernetes GPU-aware! For that you\u0026rsquo;ll be using the Intel Device Plugins for Kubernetes.\nConfiguration # Step 1: Label GPU-Enabled Nodes # First, label each GPU-equipped node so the Intel Device Plugin Operator knows where to work its magic (replace \u0026lt;node-name\u0026gt; with your own):\nkubectl label nodes \u0026lt;node-name\u0026gt; intel.feature.node.kubernetes.io/gpu=true Step 2: Install Intel Device Plugin Components # NOTE\nUpdate (July 2025): When I first documented this process, I was living the manual life – helm install this, kubectl apply that. Since then, I\u0026rsquo;ve migrated to FluxCD for GitOps-based deployments. For those interested in the automated approach, my home-ops repository shows how to deploy Intel Device Plugins declaratively. Grab the Intel Helm charts and update:\nhelm repo add intel https://intel.github.io/helm-charts/ helm repo update Deploy the Device Plugin Operator:\nhelm install --namespace=kube-system intel-device-plugins-operator intel/intel-device-plugins-operator Step 3: Configure the GPU Device Plugin # Create values.yaml to define your sharing strategy:\nname: i915 sharedDevNum: 1 # Maximum pods per GPU nodeFeatureRule: false # Disable automatic node feature discovery Deploy the GPU plugin with your created config:\nhelm install --namespace=kube-system intel-device-plugins-gpu intel/intel-device-plugins-gpu -f values.yaml Using GPU Resources # Resource Requests # You can now configure pods to request GPU resources through Helm charts or Deployment manifests:\nresources: limits: gpu.intel.com/i915: \u0026#34;1\u0026#34; requests: gpu.intel.com/i915: \u0026#34;1\u0026#34; Node Selection # Got a mixed cluster? Force GPU workloads to GPU enabled nodes:\nnodeSelector: intel.feature.node.kubernetes.io/gpu: \u0026#34;true\u0026#34; Final Check # When you deployed your first application with a resource request for a GPU, verify within the container by executing:\nls /dev/dri Sweet success looks (again) like this:\nby-path card0 renderD128 Congratulations! You\u0026rsquo;ve just pulled off the triple axel of virtualization: GPU passthrough from host to VM to container. Your media stack is now hardware-accelerated, your streams are smooth, and somewhere, a CPU is breathing a sigh of relief. Happy transcoding!\n","date":"26 May 2025","externalUrl":null,"permalink":"/guides/igpu-passthrough/configuring-kubernetes/","section":"Guides","summary":"","title":"Configuring Kubernetes","type":"guides"},{"content":"","date":"21 July 2025","externalUrl":null,"permalink":"/tags/about/","section":"Tags","summary":"","title":"About","type":"tags"},{"content":"","date":"21 July 2025","externalUrl":null,"permalink":"/","section":"byKaj","summary":"","title":"byKaj","type":"page"},{"content":"","date":"21 July 2025","externalUrl":null,"permalink":"/tags/colophon/","section":"Tags","summary":"","title":"Colophon","type":"tags"},{"content":"","date":"21 July 2025","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"26 May 2025","externalUrl":null,"permalink":"/tags/gpu/","section":"Tags","summary":"","title":"Gpu","type":"tags"},{"content":"","date":"26 May 2025","externalUrl":null,"permalink":"/tags/guide/","section":"Tags","summary":"","title":"Guide","type":"tags"},{"content":"This section contains all the guides available.\n","date":"26 May 2025","externalUrl":null,"permalink":"/guides/","section":"Guides","summary":"","title":"Guides","type":"guides"},{"content":"","date":"26 May 2025","externalUrl":null,"permalink":"/tags/intel/","section":"Tags","summary":"","title":"Intel","type":"tags"},{"content":" Share the GPU Love: Intel iGPU Split Passthrough to Kubernetes Containers Let\u0026rsquo;s face it: GPUs are too valuable to leave idle. This guide shows you how to squeeze every drop of performance from your Intel iGPU by sharing it across your entire virtualization stack – from Proxmox host to Kubernetes pods. Your media servers will thank you, your power bill will love you, and yes, you\u0026rsquo;ll still have a console to admire your work. Just don\u0026rsquo;t expect to train your next AI model with this setup – leave those workloads to the big GPUs.\n","date":"26 May 2025","externalUrl":null,"permalink":"/guides/igpu-passthrough/","section":"Guides","summary":"Share the GPU Love: iGPU Split Passthrough to Kubernetes Containers","title":"Intel iGPU Split Passthrough","type":"guides"},{"content":"","date":"26 May 2025","externalUrl":null,"permalink":"/series/intel-igpu-split-passthrough/","section":"Series","summary":"","title":"Intel IGPU Split Passthrough","type":"series"},{"content":"","date":"26 May 2025","externalUrl":null,"permalink":"/tags/k3s/","section":"Tags","summary":"","title":"K3s","type":"tags"},{"content":"","date":"26 May 2025","externalUrl":null,"permalink":"/tags/k8s/","section":"Tags","summary":"","title":"K8s","type":"tags"},{"content":"","date":"26 May 2025","externalUrl":null,"permalink":"/tags/kubernetes/","section":"Tags","summary":"","title":"Kubernetes","type":"tags"},{"content":"","date":"26 May 2025","externalUrl":null,"permalink":"/tags/proxmox/","section":"Tags","summary":"","title":"Proxmox","type":"tags"},{"content":"","date":"26 May 2025","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","date":"26 May 2025","externalUrl":null,"permalink":"/tags/talos/","section":"Tags","summary":"","title":"Talos","type":"tags"},{"content":"","date":"26 May 2025","externalUrl":null,"permalink":"/tags/virtualization/","section":"Tags","summary":"","title":"Virtualization","type":"tags"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"}]